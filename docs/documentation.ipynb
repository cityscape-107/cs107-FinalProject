{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-dbda2138-5e9a-49b7-a9a1-0499484e81c8",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Automatic Differentiation, or Algorithmic Differentiation, is a term used to describe a collection of techniques that can be used to calculate the derivatives of complicated functions. Because derivatives play a key role in computational analyses, statistics, and machine and deep learning algorithms, the ability to quickly and efficiently take derivatives is a crucial one. Other methods for taking derivatives, however, including Finite Differentiation and Symbolic Differentiation have drawbacks, including extreme slowness, precision errors, inaccurate in high dimensions, and memory intensivity.  Automatic differentiation addresses many of these concerns by providing an exact, high-speed, and highly-applicable method to calculate derivatives. Its importance is evidenced by the fact that it is used as a backbone for TensorFlow, one of the most widely-used machine learning libraries. In this project, we will be implementing an Automatic Differentiation library that can be used as the basis for analysis methods, including an Optimization and Newton‚Äôs Method extension that we will illustrate. \n",
    "\n",
    "## Background\n",
    "\n",
    "The principal concept that is going to be leveraged through automatic differentiation is that we will construct the point derivative of every function based on how a function can be decomposed into elementary operations. Computing the derivatives of these different atoms will subsequently enable us to be able to construct derivatives of a wide range of real-valued functions. Therefore, the derivative of every function can be deduced from simple laws : how to derive basic functions (or atoms) and how to handle the derivates on basic operations of functions. This is explained in the following table. We first present the atoms and then present how to handle the derivative on basic operations on functions. Here, x is a real variable and u and v are functions. \n",
    "\n",
    "|Atom function   |   Derivative |\n",
    "|:--------------:|:-------------:|\n",
    "|<img src=\"https://render.githubusercontent.com/render/math?math=x^r\">|<img src=\"https://render.githubusercontent.com/render/math?math=r*x^{r-1}\">|\n",
    "|<img src=\"https://render.githubusercontent.com/render/math?math=ln(x)\">|<img src=\"https://render.githubusercontent.com/render/math?math=\\frac{1}{x}\">|\n",
    "|e^x|e^x|\n",
    "|cos(x)|-sin(x)|\n",
    "|sin(x)|cos(x)|\n",
    "|u+v|u'+v'|\n",
    "|uv|u'v+uv'|\n",
    "|<img src=\"https://render.githubusercontent.com/render/math?math=\\frac{u}{v}\">|<img src=\"https://render.githubusercontent.com/render/math?math=\\frac{u'v-uv'}{v^2}\">|\n",
    "\n",
    "$$Table 1.$$\n",
    "\n",
    "\n",
    "Now that we know how to compute the derivatives of atoms and how to handle derivatives on basic operations of functions, we want to visualize how can a function be decomposed into thse basic operations.\n",
    "An important visualization of how a function can be decomposed into several elementary operations is the computational graph. \n",
    "\n",
    "For instance, we are going to draw the graph of the function <img src=\"https://render.githubusercontent.com/render/math?math=[f(x,y) =exp(-(sin(x)-cos(y))**2)]\">\n",
    "\n",
    "\n",
    "![images/imagesComputational_graph.jpeg](images/Computational_graph.jpeg)\n",
    "\n",
    "Therefore, the resulting quantity of interest can be explicitely expressed as a composition of several functions. In order to compute the derivative of these successive compositions, we are going to leverage a powerful mathematical tool: the **chain rule**.\n",
    "A simple version of the chain rule can be expressed as follows : for <img src=\"https://render.githubusercontent.com/render/math?math=$f$\"> and <img src=\"https://render.githubusercontent.com/render/math?math=$g$\"> two functions, \n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=$[f(g)]' = g'*f'(g)$\">\n",
    "\n",
    "Therefore, from the computational graph we have seen above, we can express the derivative of the function encoded at every node by computing the derivative of this elementary operation and multiplyingby the derivative of the inner function. We know that we are able to compute the derivative of the elementary operation from the derivative of the different atom functions. \n",
    "Now, the question is to get the derivative of the inner function, that represents all the composition of the different operations encoded at every node until the current node. We do this iteratively, by applying at every node the chain rule with the previous composition operations. \n",
    "This suite of operations is encoded on the trace table. \n",
    "\n",
    "\n",
    "![images/Evaluation_table.png](images/Evaluation_table.png)\n",
    "\n",
    "Therefore, from the previous points, we see that we will be able to compute value of the gradient of a function evaluated on a point by iteratively applying the chain rule at every operation node and leveraging a set of basic derivatives and operation on derivatives.\n",
    "\n",
    "\n",
    "Let's now move on to a brief background of our extension: optimization. Specifically, Gradient Descent. Gradient Descent is the preferred way to optimize neural networks and other Machine Learning algorithms. There are tens of varients of the algorithm - we have demonstrated 3 of these, namely Adam, Stochastic Gradient Descent, and RMS Prop. \n",
    "\n",
    "Gradient Descent, in general, is a way to minimize a certain objective function <img src=\"https://render.githubusercontent.com/render/math?math=$J(\\theta)$\"> that is parametrized by a model's parameters <img src=\"https://render.githubusercontent.com/render/math?math=$\\theta \\in R^d$\">. Essentially, we follow the direction of the slope of the objective function surface downwards until we reach a local minimum ( a valley). More specifically, it updates the parameters in the _opposite_ direction of the gradient of the objective function <img src=\"https://render.githubusercontent.com/render/math?math=$\\nabla_\\theta J(\\theta)$\"> with respect to our parameters. We define a _learning rate_ <img src=\"https://render.githubusercontent.com/render/math?math=$\\eta$\"> that determines the size of the steps we take in this direction to reach a local minimum. \n",
    "\n",
    "\n",
    "You can see a basic visualization of this process in the graphic below. \n",
    "\n",
    "![images/gradientDescentPic.jpeg](images/gradientDescentPic.jpeg)\n",
    "\n",
    "Additionally, a more 3D visualization of this process can be seen in the graphic below. \n",
    "\n",
    "![images/3dgradientDescentPic.jpg](images/3dgradientDescentPic.jpg)\n",
    "\n",
    "We can use an Adaptive learning rate Œ∑ allows us to change the size of our steps per iteration based on the current gradient. The graph below compares the error of models that use a static learning rate versus one that uses an adaptive learning rate over a certain number of epochs. As one can see, the adaptive learning rate reaches a lower error more quickly. \n",
    "\n",
    "![images/adaptiveLearningRateGraph.png](images/adaptiveLearningRateGraph.png)\n",
    "\n",
    "We also deal with momentum, which helps accelerate gradient descent and reduces the oscillation, as one can see in the image below. It does this by adding a fraction of the update vector of the past time step to the current update vector. \n",
    "\n",
    "![images/momentum.png](images/momentum.png)\n",
    "\n",
    "  A version of this algorithm is present in almost every advanced machine learning and deep learning library, and it's strength is a testament to how powerful Automatic Differentiation is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-b4c34eb9-80d2-4543-aa3b-de1d98e10b6a",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "## How to use\n",
    "\n",
    "- The url to the project is: https://github.com/cityscape-107/cs107-FinalProject\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-c9877330-ea58-4c3c-9775-efe2796ea985",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Our CityAD package is easy to use. Users will have easy access through PyPI.\n",
    "\n",
    "**Steps to install our package:**\n",
    "\n",
    "\n",
    "We recommend first setting up an virtual environment such as Anaconda. (This official document gives a good explanation on virtual environment: https://docs.python.org/3/tutorial/venv.html)\n",
    "\n",
    "- Install Anaconda, you can download it for free at: https://www.anaconda.com/products/individual\n",
    "\n",
    "- Create a virtual environment:\n",
    "```\n",
    "#in command line\n",
    "conda -V\n",
    "conda create -n Cityscape\n",
    "activate Cityscape\n",
    "```\n",
    "- Then, 'pip install' our package.\n",
    "```\n",
    "pip install CityAD\n",
    "```\n",
    "\n",
    "- Inclusivity: We fully understand that some users might not have access to PyPI. Our package is structured in a way that users can make it work even with the simplest 'copy and paste.' In these situations, download our package directly from github: https://github.com/cityscape-107/cs107-FinalProject, copy and paste `ADmulti.py` and `Optimizer.py` to your working folder. \n",
    "- Lastly, if any of above does not work for you, please email our maintenance specialist at cityad.helpline@gmail.com for further installation assistance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-73d7941a-6094-4d60-bcb5-dc0d412517f1",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "We will present the usage of our `Automatic Differentiation (ADmulti)` and `Optimizer` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00003-37f2ef76-88fb-4a4d-a91e-e7e7bf19a1f9",
    "deepnote_cell_type": "code",
    "execution_millis": 5,
    "execution_start": 1607792146648,
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "b65dd23b"
   },
   "outputs": [],
   "source": [
    "# once you installed\n",
    "from AD.ADmulti import AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-7cfd62d3-8e33-4320-80f0-09c52caba232",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "**How to use Automatic Differentiation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-05c241ff-5609-42ad-94fc-1066cd72f7b6",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "First, consider a scalar variable $x$ and the function:\n",
    "$$\n",
    "f=x^{2}+2 x+1\n",
    "$$\n",
    "where,$$x=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00006-0e49d4f0-0ccd-4228-8e99-d6733516a35c",
    "deepnote_cell_type": "code",
    "execution_millis": 4,
    "execution_start": 1607792146658,
    "output_cleared": false,
    "source_hash": "d1415d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Value is:\n",
      "[[4.]], \n",
      "Jacobian is:\n",
      "[[4.]], \n",
      "Name is:\n",
      "['x']\n"
     ]
    }
   ],
   "source": [
    "# case 1\n",
    "# define a scalar variable\n",
    "x=AD(1,1,'x')\n",
    "f=x**2+2*x+1\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-b2705031-109a-4de5-8933-5b874cdebbe6",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Next, consider a more complicated vector function, with variable $x=4,y=1,z=3$:\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "f_{1}=x^{2}+ln(x)+y-z \\\\\n",
    "f_{2}=2 y-z \\\\\n",
    "f_{3}=\\sin (x)+z\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00008-56914bd8-a713-4d2e-8103-0a02bf1a22ab",
    "deepnote_cell_type": "code",
    "execution_millis": 5,
    "execution_start": 1607792146664,
    "output_cleared": false,
    "source_hash": "66f45f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Value is:\n",
      "[[15.38629436]\n",
      " [-1.        ]\n",
      " [ 2.2431975 ]], \n",
      "Jacobian is:\n",
      "[[ 8.25       -1.          1.        ]\n",
      " [ 0.         -1.          2.        ]\n",
      " [-0.65364362  1.          0.        ]], \n",
      "Name is:\n",
      "['x', 'z', 'y']\n"
     ]
    }
   ],
   "source": [
    "# case 2\n",
    "# define a (matrix-style) function system\n",
    "x=AD(4,1,'x')\n",
    "z=AD(3,1,'z')\n",
    "y=AD(1,1,'y')\n",
    "\n",
    "f1=AD([x**2+x.ln()+y-z])\n",
    "f2=AD([2*y-z])\n",
    "f3=AD([x.sin()+z])\n",
    "\n",
    "# v is a function system\n",
    "v=AD([f1,f2,f3])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-e143d936-0b82-440e-8b6d-18b1d18cf270",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "If the value and Jacobian is not well ordered, we provide a method to sort them, so that you can do matrix operations more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00010-0f9a9778-141b-4b8b-b2a7-e089f291bcb0",
    "deepnote_cell_type": "code",
    "execution_millis": 2,
    "execution_start": 1607792146672,
    "output_cleared": false,
    "source_hash": "eeb206ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Value is:\n",
      "[[15.38629436]\n",
      " [-1.        ]\n",
      " [ 2.2431975 ]], \n",
      "Jacobian is:\n",
      "[[ 8.25        1.         -1.        ]\n",
      " [ 0.          2.         -1.        ]\n",
      " [-0.65364362  0.          1.        ]], \n",
      "Name is:\n",
      "['x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "v.sort(['x','y','z'])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-4a5c070c-19a6-4339-92d1-dba863c5dbd3",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "if you only need one of the Value, Jacobian, and Name, do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00012-99af6f34-99e1-40e0-a351-2503aac0c3c7",
    "deepnote_cell_type": "code",
    "execution_millis": 1,
    "execution_start": 1607792146700,
    "output_cleared": false,
    "source_hash": "2c0bb998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15.38629436]\n",
      " [-1.        ]\n",
      " [ 2.2431975 ]]\n"
     ]
    }
   ],
   "source": [
    "print(v.val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00013-b5a977c7-10bb-4395-aa8e-ab741e0d1b35",
    "deepnote_cell_type": "code",
    "execution_millis": 1,
    "execution_start": 1607792146701,
    "output_cleared": false,
    "source_hash": "58f53214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.25        1.         -1.        ]\n",
      " [ 0.          2.         -1.        ]\n",
      " [-0.65364362  0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(v.der)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00014-8f03ed2d-bf33-4fb0-b986-83616e757908",
    "deepnote_cell_type": "code",
    "execution_millis": 0,
    "execution_start": 1607792146702,
    "output_cleared": false,
    "source_hash": "e8cdbb7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(v.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-21c42acb-aa6e-48d5-8aa1-34a7ae5c93d4",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "**How to use the Optimizer extension?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00016-adf38dfa-a1e6-4721-b450-fea6ce199e2c",
    "deepnote_cell_type": "code",
    "execution_millis": 5,
    "execution_start": 1607792146705,
    "output_cleared": false,
    "source_hash": "e42488c9"
   },
   "outputs": [],
   "source": [
    "from Extensions.Optimizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-695b035a-0887-4830-ae31-28ae3906dd48",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "First case, scalar function of a scalar input, without any initial points and 10 random restarts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00017-6285ca2b-444f-49ed-8542-a7434dffd4eb",
    "deepnote_cell_type": "code",
    "execution_millis": 385,
    "execution_start": 1607792146712,
    "output_cleared": false,
    "source_hash": "1eb45cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal point found for minimization is  -0.0001433746534262066 which value is  2.0000000205562913\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: x ** 2 + 2\n",
    "adam = Adam(f, random_restarts=10)\n",
    "adam.descent()\n",
    "z = adam.global_optimizer\n",
    "print('The optimal point found for minimization is ', *z, 'which value is ', f(*z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-5a5d1d19-cbdb-45c3-a574-c9c610a05cf7",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Second case, scalar function of two scalar inputs, without initial points and 10 random restarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00018-dae471f2-5f52-4824-a47a-90510b0ef7cc",
    "deepnote_cell_type": "code",
    "execution_millis": 2075,
    "execution_start": 1607792147105,
    "output_cleared": false,
    "source_hash": "78633885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal point found for minimization is  -1.992158958773848e-88 2.999793507393831 which value is  2.0000000426391966\n"
     ]
    }
   ],
   "source": [
    "f = lambda x, y: x ** 2 + 2 + (y-3)**2\n",
    "adam = Adam(f, random_restarts=10)\n",
    "adam.descent()\n",
    "z = adam.global_optimizer\n",
    "print('The optimal point found for minimization is ', *z, 'which value is ', f(*z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-c5fc650c-1269-4e92-a149-0fe4d6285a23",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Third case, scalar function of several scalar inputs, with initial points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00023-8bd6f827-4797-4452-bab7-a36b371c0c68",
    "deepnote_cell_type": "code",
    "execution_millis": 2380,
    "execution_start": 1607792149194,
    "output_cleared": false,
    "source_hash": "135024c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal point found for minimization is  2.415537745933444e-08 2.999790003389618 which value is  2.000000044098577\n"
     ]
    }
   ],
   "source": [
    "f = lambda x, y: x ** 2 + 2 + (y-3)**2\n",
    "adam = Adam(f, random_restarts=10, init_points = [4, -2])\n",
    "adam.descent()\n",
    "z = adam.global_optimizer\n",
    "print('The optimal point found for minimization is ', *z, 'which value is ', f(*z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-02a73bac-423a-47d3-84c2-75e56c279dde",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Fourth case: the user wishes to optimize a vector function, with 20 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00026-7017d9f6-6e0c-4031-91fc-9abf105c52f2",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "91993351",
    "tags": []
   },
   "outputs": [],
   "source": [
    "f = lambda v: np.sum(v**2)\n",
    "adam = Adam(f, random_restarts=10, dimensions=20)\n",
    "adam.descent()\n",
    "z = adam.global_optimizer\n",
    "print('The optimal point found for minimization is ', *z, 'which value is ', f(np.array(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-499f1978-c325-4011-8ba3-d1c945c76029",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "Visualization: the user wishes to compare the performances of the 3 optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00029-85e43e27-2a25-4c96-81ae-66ff480dd281",
    "deepnote_cell_type": "code",
    "execution_millis": 2551,
    "output_cleared": false,
    "source_hash": "5aa7c327",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = lambda x, y: x**2+y**2\n",
    "stochasticgd = sgd(f, random_restarts=1)\n",
    "stochasticgd.descent()\n",
    "trace_sgd = np.array(stochasticgd.trace)\n",
    "trace_values_sgd = np.array(stochasticgd.trace_values)\n",
    "adam = Adam(f, random_restarts=1)\n",
    "adam.descent()\n",
    "trace_adam = np.array(adam.trace)\n",
    "trace_values_adam = np.array(adam.trace_values)\n",
    "rms = RMSProp(f, random_restarts=1)\n",
    "rms.descent()\n",
    "trace_rms = np.array(rms.trace)\n",
    "trace_values_rms = np.array(rms.trace_values)\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = plt.axes(projection='3d')\n",
    "X = np.arange(-1, 1, 0.10)\n",
    "Y = np.arange(-0.5, 1, 0.10)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z = f(X, Y)\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.plot_wireframe(X, Y, Z, color='gold')\n",
    "ax.scatter(trace_sgd[:, 0], trace_sgd[:, 1], trace_values_sgd[1:, 0], s=100, color='lightcoral', label='Adam')\n",
    "ax.scatter(trace_adam[:, 0], trace_adam[:, 1], trace_values_adam[1:, 0], s=100, color='lightblue', label='Sgd')\n",
    "ax.scatter(trace_rms[:, 0], trace_rms[:, 1], trace_values_rms[1:, 0], s=100, color='lightpink', label='Rms')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-0.5, 1)\n",
    "ax.legend()\n",
    "ax.set_title('Visualization of the performances of 3 the different Optimizers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-ce45c35e-24f8-48fd-87fa-1ab32d61e9bd",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Software organization\n",
    "#### 1. Modules \n",
    "Our automatic differentiation package (named `Cityscape-107`) will consist of two modules:\n",
    " - A main module (`AD`) for the basic requirements of automatic differentiation. \n",
    " - An additional module(`optimization`) will be an extension of the basic requirements. The optimization method will be used to maximize or minimize a given function making use of its derivative and maybe also making use of the rootfinder method in order to find the values for which the sderivatives become zero. \n",
    "\n",
    "\n",
    "The `AD` module has a main file ADmulti containing the main class called AD. It is initialized with a two arguments (value and value for the derivative) and they are stored as `self.val` and `self.der` attributes. There are several functions including the overloaded dunder methods/operations `__add__`, `__radd__`,`__mul__`, `__sub__`, `__pow__`, etc. as well as some basic functions `sin`, `cos`, `exp`, etc.\n",
    "\n",
    "\n",
    "#### 2. Directory Structure \n",
    "All the modules will be found in the directory `Cityscape-107`, under subdirectories with the name of the module. There will also be a directory for `tests`, as well as `examples` and `documentation`. Additional documentation will be also provided for each individual module.\n",
    "\n",
    "The main directory will also include files like the `.travis.yml`, `.codecov.yml`, `setup.py`, `README.md`, `LICENSE.txt` and any other necessary files.\n",
    "\n",
    "The structure will be similar to the following example:\n",
    "\n",
    "\n",
    "```python\n",
    "Cityscape-107/\n",
    "        cs107-FinalProject/ #main directory \n",
    "                __init__.py  \n",
    "                AD/ #main module\n",
    "                        __init__.py\n",
    "                        ADmulti.py\n",
    "                        test_ADmulti.py\n",
    "                        driver.py\n",
    "\n",
    "                optimization/ #extension \n",
    "                        __init__.py\n",
    "                        optimizer.py\n",
    "                        Test_Optimizer.py\n",
    "\n",
    "                docs/ #documentation\n",
    "                        images/ #contain images used in docs\n",
    "                        prev_milestone_docs/ #previous milestone docs \n",
    "                        documentation.md\n",
    "\n",
    "                .travis.yml\n",
    "                README.md\n",
    "                .codecov.yml\n",
    "                requirements.txt\n",
    "                setup.py\n",
    "                LICENSE.txt #terms of distribution\n",
    "                ...\n",
    "```\n",
    "\n",
    "#### 3. Distribution\n",
    "We will distribute our package using `PyPI`. The files  `setup.py`, `setup.cfg`, `LICENSE.txt` and `README.md` that are outside of the `Cityscape-107` package folder are necessary for PyPI to work. \n",
    "\n",
    "The file `setup.py` will contain important information like:\n",
    " - the `name` and `version` of the package.\n",
    " - `download_url` (GitHub url).\n",
    " - `install_requires` (list of dependencies).\n",
    " \n",
    "By uploading our package to `PyPI` it will be easy to install just by simply writting:\n",
    "\n",
    "       $ pip install CityAD\n",
    "\n",
    "#### 4. Testing \n",
    "We will use the continuous integration tool `Travis-CI` linked to our GitHub project to automatically test changes before integrating them into the project. This will ensure that new changes are merged only if they pass the tests and do not break our code. Our tests are located in the directories with the files they are covering. The `test_ADmulti.py` file, for example, which contains many tests for the AD function, is in the `AD` directory alongside `ADmulti.py`. The `test_Optimizer.py` file, containing the test for our Extension is located into the `Extensions` directory. \n",
    "\n",
    "Additionally, `Codecov` will provide coverage reports of the tests performed i.e. the percentage of our code that the tests actually tested. After tests are successfully run by `Travis-CI` a report is sent to `Codecov`, which will show the test coverage of the code in our project repository. \n",
    "\n",
    "#### 5. Packaging: \n",
    "\n",
    "We package our project following Python's official guide: https://packaging.python.org/tutorials/packaging-projects/. `setuptools` and `wheel` are used to package the project.\n",
    "\n",
    "After packaging, it is uploaded to PyPI, so that users have easy access to it with `pip install CityAD`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Implementation \n",
    "\n",
    "#### 1. Core Class\n",
    "\n",
    "In order to implement our Forward Mode, our core class was the AD class. \n",
    "The AD class was a representation of a Node in our computational graph. It holds as attributes a value and a derivative, which are computed as in the trace table. Furthermore, in order for our class to handle multi-dimensional inputs, \n",
    "we needed to keep track of the dimensionalities. THerefore, we added an attribute called 'names' that would allow us to encode the location of every dimension inside numpy arrays.\n",
    "\n",
    "#### 2. Core Data Structure \n",
    "\n",
    "The core Data Structure we leveraged is numpy arrays. We made the choice to encode values and derivatives as numpy arrays because they allowed us to efficiently perform basic operations on them, without \n",
    "having to loop over the datastructure and without having to worry about the dimensionality of our input. This allowed us to have a much more efficient and natural way of implementing our over-loading of basics operations.\n",
    "Moreover, we wanted value and derivatives to have a static dimensions: we did not need to add anything to those arrays (except concatenating, which is more efficient with arrays than lists). All of these reasons have motivated the use of numpy arrays for values and derivatives.\n",
    "Regarding the attribute names, we needed to have a more flexible data structure, in order to be able to deal with the case when we have different Ad variables that have been defined in terms of different dimensions. This motivated the use of lists. \n",
    "  \n",
    "\n",
    "#### 3. Important attributes of the class\n",
    "\n",
    "The important attributes of the AD class are value, derivative and names. \n",
    "As we mentioned earlier, val is the very value of the function at the variable which it is evaluated. We need this attribute in the AD class because the derivative of some elmentary functions might depend on their value. Values was a numpy array that allowed easy operations between two AD variables. The input types should be integer, float, numpy array or lists.\n",
    "\n",
    "The derivative of a variable is needed in order to instantiate AD variable (‚Äúseed‚Äù of vector) in order to specify we aren‚Äôt dealing with a constant.\n",
    "\n",
    "The name attribute is used to manage the location of the derivatives with respect to each variable. We do not keep track of the ‚Äòorder‚Äô of dimensions wanted by the user - we just make sure to not mix up dimensions between them. Moreover, imposing an order on dummy variables does not make much sense (f(x, y, z) = f(z, y, x) mathematically). \n",
    "Therefore, when the user wishes to leverage the Jacobian, they can specify the order in which they want it using the sort() function which takes an an input the order of the names of the variables. \n",
    "\n",
    "\n",
    "#### 4. External Dependencies\n",
    "\n",
    "We tried to keep the external dependencies at a minimum. There are two reasons for that:\n",
    "- User Convenience (the user does not need to install 100 packages to run our code)\n",
    "- Implementation convenience (every external dependency has its own syntax in a way, and we wanted to be consistent \n",
    "in our implementation regarding design)\n",
    "\n",
    "Therefore, the only two external dependencies needed in order to run our code are: math and numpy libraries.\n",
    "\n",
    "#### 5. Elementary functions\n",
    "\n",
    "We defined several elementary functions in order to define the way AD variables would interact between each other.\n",
    "This has been done via overloading the elementary operations: addition, substraction, multiplication, division and power functions. \n",
    "For the division operation, we needed to pay extra attention to the __rtruediv__ operation, because of the asymmetry of this operation. \n",
    "Last, the power overloading was also delicate because of forbidden cases and the derivation of a function which exponent being another function.   \n",
    "We also defined the elementary functions: trig functions, inverse trig functions, hyperbolic functions, a logistic function, exponentials (including exp()), and logs to various bases (with the special case of natural logs).\n",
    "\n",
    "\n",
    "## Our Extension: Applications to Optimization, Gradient Descent, and Machine Learning \n",
    "There are several computational applications that implemenent a type of gradient descent that could harness our automatic differentiation tool. In order to apply our AD tool to a neural network, for instance, we would need to have variables that stored the weights for the layers of the network, and our AD tool would be used as a step of the backpropogation methodology. \n",
    "\n",
    "As we discussed above, Gradient Descent is an algorithm that involves a set of parameters that will minimize a loss. It's equation looks something like this: <img src=\"https://render.githubusercontent.com/render/math?math=$\\theta_{t+1} = \\theta_t - \\alpha * \\Nabla_\\theta* J$\">\n",
    "\n",
    "The algorithm that involves updating a set of parameters to minimize a loss, and is typically in the form of ùúÉ_ùë°+1=ùúÉ_ùë°‚àíùõº‚àá_ùúÉùêΩ. The gradient here is the gradient of the loss with respect to the parameters - Automatic differentiation allows us to automate the calculation of this step / these derivatives. Our file would repeatedly make use of the AD() class to calculate the derivatives! The disadvantages of automatic differentiation outweigh the advantages in this situation. \n",
    "\n",
    "![images/gradientDescentPic2.jpg](images/gradientDescentPic2.jpg)\n",
    "\n",
    "With that basic background built up (for more of an intuitive understanding, see the `Background` section of the documentation above), \n",
    "let's describe an example of where gradient descent might be used, namely the problem of predicting housing prices, i.e. the ML equivelent of \"Hello World\". Every machine learning model needs a problem T, a performance measure P, and a dataset E, from where our model can learn patterns. Let us say that our dataset has 781 data records, each of which has 3 features - size (square feet), school district (integer from 1-20) and price. \n",
    "In order to measure accuracy, we have to create a performance measure. We can use, for example, the Mean Squared Error (formula shown below - don't worry about the variables, the colored labels should sort out their meaning!)\n",
    "\n",
    "![images/meanSquaredErrorFormula.jpg](images/meanSquaredErrorFormula.jpg)\n",
    "\n",
    "Now, our goal is to build a model (a function whose parameters we are trying to find) that can take in a size and predict a housing price.\n",
    "A simplified version of our end goal function is the following: \n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=$housingPrice = weight1 * squareFootage %2B weight2*schoolDistrict\">\n",
    "\n",
    "To get started, we randomly initialize values of weight1 and weight2. Then, at each iteration, we take our dataset, feed it into our current function, and use our results to calculate our error. With that error, we can calculate the partial derivatives of the error with respect to each weight, and adjust our weights accordingly. \n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=$\\Nabla Err = [\\frac{\\partial}{\\partial W_0}, \\frac{\\partial}{\\partial W_1}]^T$\">\n",
    "\n",
    "We want our weights to update so that they lower our error in our next iteration, so we need to make them follow the _opposite direction_ of each respective gradient. We are going to change the weights by taking a small step of size <img src=\"https://render.githubusercontent.com/render/math?math=$\\eta$\"> in this opposite direction. Putting these steps together, we have our update step formula as: \n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=$W_O = W_0 - \\eta * \\frac{\\partial}{\\partial W_0}$ \">\n",
    "\n",
    "\n",
    "<img src=\"https://render.githubusercontent.com/render/math?math=$W_1 = W_1 - \\eta * \\frac{\\partial}{\\partial W_1}$\"> \n",
    "\n",
    "We continue with these iterations until we have an error that is either 0 or below a certain threshold that we set for ourselves. \n",
    "\n",
    "\n",
    "Recall the graphic that we used to visualize 3D gradient descent from above. If we imagine <img src=\"https://render.githubusercontent.com/render/math?math=$\\theta_0$\"> \n",
    "and <img src=\"https://render.githubusercontent.com/render/math?math=$\\theta_1$\"> to be our two input features, square footage and school district, this graphic makes it very easy to visualize the process we just covered!\n",
    "\n",
    "![images/3dgradientDescentPic.jpg](images/3dgradientDescentPic.jpg)\n",
    "\n",
    "Now that we've built a solid understanding of the process of gradient descent, let us explore our extension and how we built it. Our `Optimizer.py` file contains 4 classes: an `Optimizer()` class, along with three Gradient Descent Algorithms that inherit the `Optimizer()` class: `Adam(Optimizer)`, `stg(Optimizer)` (Stochastic Gradient Descent), and `RMSProp(Optimizer)`. \n",
    "\n",
    "Our `Optimizer()` class essentially builds a base gradient descent framework whose parameters (i.e. batch data sets, adaptive learning rate) can then be modified in classes that inherit it for a more customized experience. \n",
    "`Optimizer()` consists of 5 methods: \n",
    "- `__init__()`, which initializes and returns an object of the class Optimizer, allowing a user to optimize a function based on a descent method of their choice\n",
    "- `__str__()`, a tostring method\n",
    "- `produce_random_points()`, a function which allows a user to produce random initialziation points in order to start the process of gradient descent when they don't wish to individually specify them. It infers dimensionality of the inputs and returns points sampled from a gaussian distribution. \n",
    "- `annealing()`, a function which essentially allows the user to fine tune their original points so that they start off on the right foot, accelerating the optimization algorithm. At tthe moment, it only supports the quadratic function. \n",
    "- `descent()`, a function which actually runs our gradient descent algorithm to minimize a function. \n",
    "\n",
    "We then inherit `Optimizer()` into 3 other classes: \n",
    "- `Adam(Optimizer)`, which uses the default values we define for the parameters. By default, we have an adaptive learning rate for each weight that changes depending on the accumulated squared gradients until that iteration. Adam also keeps an exponentially decaying average of past gradients, similar to momentum (a method that essentially helps accelerate gradient descent and reduces the oscillation). \n",
    "- `sgd(Optimizer)`, which reverts some of our default parameters that implemented \"momentum\"  for a standard, classic gradient descent method. This is stochastic gradient descent without any of our special parameters, like momentum, adaptive learning rate, etc. \n",
    "\n",
    "- `RMSProp(Optimizer)`, which also (1) removes momentum and (2) adds an adaptive learning rate that changes depending on the accumulated squared gradients until that iteration (like in Adam). \n",
    "\n",
    "Each of these classes have `__init__()` methods and `__str__()` methods. \n",
    "\n",
    "\n",
    "#### Other Examples: \n",
    "We also implemented a driver for Newton's Root Finding Method for vector valued functions of vector variables. You can find this in the `examples` folder of our codebase. Newton's method, or the Newton-Raphson method, produces successively better approximations to the roots (or zeroes) of a real-valued function. It uses an initial guess and the function's derivative to approximate the root. \n",
    "\n",
    "\n",
    "\n",
    "## Broader Impact:\n",
    "As we discussed above, automatic differentiation has an incredible and a wide spread array of applications. And while there might not be a way to ‚Äúmisuse‚Äù the simple practice of taking derivatives, the applications in which Automatic Differentiation is used, especially Artificial Intelligence and Optimization, are ripe for misuse. Much in the same way that biased datasets lead to biased ML models and biased predictions, if our automatic differentiation library gives incorrect values or values that don‚Äôt have a high accuracy, the use of those values in real-life algorithms can lead to issues.  AI and Machine Learning, is, at its core, simply math - derivatives and gradients taken across a dataset to minimize a certain loss. If we incorrectly calculate these values, our algorithm could arrive at results that could have harmful effects on the very people they are meant to help. These algorithms range from weather prediction models to algorithms that determine bail amounts and the length of someone‚Äôs prison sentence. This comes back to an idea that our society is grappling with at the moment - while Automatic Differentiation and its uses in optimization (the extension that we implemented) and machine learning algorithms are powerful, we must take careful steps that every major decision the algorithm makes is human-reviewed by a diverse and responsible board with a knowledge of the subject area. Algorithms are only as powerful as the people who design them, and we can ensure that they are used in the best way possible by creating a culture of responsibility and ethical-based review in our codebase and community. \n",
    "\n",
    "## Software Inclusivity: \n",
    "We developed this project using github version control, a system that is often quite difficult and confusing to understand in the very beginning of stages of trying to use it. People who might not have access to a robust and in-depth computer science education might find it difficult to contribute to this project because they aren‚Äôt familiar with branching, committing, pushing, and pulling from github. These groups include several underrepresented minorities, including women, Black, Latinx, and Native Americans. This also includes people in rural and urban areas who might not have access to a CS education that includes concepts like git and version control. As someone who learned about these concepts quite late in her CS education as well, I can testify to the fact that ‚Äúsoftware development‚Äù as a practice can often be quite intimidating and can keep bright and talented people from pursuing the field because of how much prior knowledge of ‚Äúarcane‚Äù (but necessary) things like git. \n",
    "\n",
    "Once a developer has made themselves comfortable with the codebase and with the practice of version control, we have a very fair and open system of code contribution. After making a branch, developers can create Pull Requests that are approved by either a few members of the team (if the PR involves a small change, such as a bug fix or comment) or all members of the team (if the PR involves major changes such as a new feature, test, or method). If reviewers have any questions, they leave them in the Comments space of the PR and the developer can respond to them as needed. If our organization was bigger, we would make sure to hire developers who are diverse in age, race, gender, and sexual orientation in order to make everyone feel completely comfortable adding radical features, challenging their peers and having respectful disagreements, and overall contributing to a more robust and dynamic codebase and product. \n",
    "\n",
    "\n",
    "## Future\n",
    "In line with the tech community's growing emphasis on / acknowledgement of the fact that as developers and researchers, we must be acutely aware of the impacts of our work, it would be very cool to create another extension of our AD package that creates a \"Effects of Biased Data\" visualizer. Although there is a growing interest in post-processing bias mitigation methods, the most common and spoken-about cause of AI and ML model bias is rooted in biased, unrepresentative datasets. Google, IBM, and Microsoft all came under fire for releasing computer vision models (which also use backpropogation and automatic differentiation, another possible future extension) that were unable to recognize african american women. The main reason for this inability? A dataset that didn't have nearly enough women of color as compared to white men and women. \n",
    "\n",
    "An \"Effects of Biased Data Visualizer\" could use an Automatic Differentiation core to visually show the changing accuracies of computer vision models as the datasets on which they are trained become more diverse and representative of the world around us. In the same way that we built an \"Optimizer\" extension with an AD core, we could build a Computer Vision extension and then create a visualization that shows (perhaps in graph form) how a model's ability to perform on images of underrepresented minorities changes as its dataset becomes more diverse. \n",
    "\n",
    "To consider some future features for our optimization extension, we think of a more refined optimization tool. One of the risks of running gradient descent is the chance that you might find yourself ‚Äústuck‚Äù in a local optima instead of a global one. (Graphic visually explaining this shown below). One way to prevent this is (mini) batch stochastic gradient descent, which is something we could work on in the future. \n",
    "Additionally, we would like to find a way to tune initialization points for functions that are not quadratic, maybe leveraging Markov Chain Monte Carlo sampling. \n",
    "\n",
    "\n",
    "![images/localGlobalMinima.png](images/localGlobalMinima.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Additionally, thinking beyond extensions rooted in computer science, Automatic Differentiation has a whole host of applications in the natural sciences! We could implement a specific automatic differentiation technique called \"interface contraction\" that is commonly used in biostatistical analysis. Interface contraction makes derivative computation more effecient by taking advantage of the fact that the number of variables passed between subroutines is often very small compared to the number of variables with respect to which you want to differentiate. It has been famously used in a study analyzing the relationships of age and dietary intake with risk of developing breast cancer in a cohort of 89,538 nurses. Implementing this type of AD for use in biostatistics would be fascinating and an excellent way to show the many applications of Automatic Differentiation. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "f51cb65d-8e2d-41d2-b9da-9ac7217ffa50",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
